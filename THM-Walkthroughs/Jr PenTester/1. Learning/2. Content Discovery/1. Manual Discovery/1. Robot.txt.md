
There are multiple places you can start checking to understand hidden directories and files.

The `robots.txt` tells the search engine which sites need to be restricted from being shown upon search or ban certain search engines from crawling the website altogether.
This usually contains a list of pages that the owner does not want the common user to access.

To use it, try suffixing the url with `/robots.txt`.